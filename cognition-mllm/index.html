<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="What is the Visual Cognition Gap between Humans and Multimodal LLMs?">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Recently, Multimodal Large Language Models (MLLMs) and Vision Language Models (VLMs) have shown great promise in language-guided perceptual tasks such as recognition, segmentation, and object detection. However, their effectiveness in addressing visual cognition problems that require high-level multi-image reasoning and visual working memory is not well-established. One such challenge is matrix reasoning - the cognitive ability to discern relationships among patterns in a set of images and extrapolate to predict subsequent patterns. This skill is crucial during the early neurodevelopmental stages of children. Inspired by the matrix reasoning tasks in Raven's Progressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC), we propose a new dataset MaRs-VQA to evaluate the visual cognition capability of MLLMs and compare their performance with existing human visual cognition studies. Based on the training data of MaRs-VQA, we also finetune a baseline model Qwen2-VCog with multi-stage cognition reasoning annotations. Our comparative experiments with different baselines reveal a gap between MLLMs and human intelligence, highlighting the visual cognitive limitations of current MLLMs. We believe that the public release of MaRs-VQA and the Qwen2-VCog baseline model will drive progress toward the next generation of MLLMs with human-like visual cognition abilities.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Computer Vision and Pattern Recognition, Artificial Intelligence">
  <!-- TODO: List all authors -->
  <meta name="author" content="Xu Cao, Yifan Shen, Bolin Lai, Wenqian Ye, Yunsheng Ma, Joerg Heintz, Jintai Chen, Meihuan Huang, Jianguo Cao, Aidong Zhang, James M. Rehg">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="PediaMed AI">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="What is the Visual Cognition Gap between Humans and Multimodal LLMs?">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Recently, Multimodal Large Language Models (MLLMs) and Vision Language Models (VLMs) have shown great promise in language-guided perceptual tasks such as recognition, segmentation, and object detection. However, their effectiveness in addressing visual cognition problems that require high-level multi-image reasoning and visual working memory is not well-established. One such challenge is matrix reasoning - the cognitive ability to discern relationships among patterns in a set of images and extrapolate to predict subsequent patterns. This skill is crucial during the early neurodevelopmental stages of children. Inspired by the matrix reasoning tasks in Raven's Progressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC), we propose a new dataset MaRs-VQA to evaluate the visual cognition capability of MLLMs and compare their performance with existing human visual cognition studies. Based on the training data of MaRs-VQA, we also finetune a baseline model Qwen2-VCog with multi-stage cognition reasoning annotations. Our comparative experiments with different baselines reveal a gap between MLLMs and human intelligence, highlighting the visual cognitive limitations of current MLLMs. We believe that the public release of MaRs-VQA and the Qwen2-VCog baseline model will drive progress toward the next generation of MLLMs with human-like visual cognition abilities.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://pediamed.ai">

  <!-- Twitter -->
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@pediamedai">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="What is the Visual Cognition Gap between Humans and Multimodal LLMs?">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="Recently, Multimodal Large Language Models (MLLMs) and Vision Language Models (VLMs) have shown great promise in language-guided perceptual tasks such as recognition, segmentation, and object detection. However, their effectiveness in addressing visual cognition problems that require high-level multi-image reasoning and visual working memory is not well-established. One such challenge is matrix reasoning - the cognitive ability to discern relationships among patterns in a set of images and extrapolate to predict subsequent patterns. This skill is crucial during the early neurodevelopmental stages of children. Inspired by the matrix reasoning tasks in Raven's Progressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC), we propose a new dataset MaRs-VQA to evaluate the visual cognition capability of MLLMs and compare their performance with existing human visual cognition studies. Based on the training data of MaRs-VQA, we also finetune a baseline model Qwen2-VCog with multi-stage cognition reasoning annotations. Our comparative experiments with different baselines reveal a gap between MLLMs and human intelligence, highlighting the visual cognitive limitations of current MLLMs. We believe that the public release of MaRs-VQA and the Qwen2-VCog baseline model will drive progress toward the next generation of MLLMs with human-like visual cognition abilities.">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="What is the Visual Cognition Gap between Humans and Multimodal LLMs?">
  <meta name="citation_author" content="Cao, Xu">
  <meta name="citation_author" content="Shen, Yifan">
  <meta name="citation_author" content="Lai, Bolin">
  <meta name="citation_author" content="Ye, Wenqian">
  <meta name="citation_author" content="Ma, Yuansheng">
  <meta name="citation_author" content="Heintz, Joerg">
  <meta name="citation_author" content="Chen, Jintai">
  <meta name="citation_author" content="Huang, Meihuan">
  <meta name="citation_author" content="Cao, Jianguo">
  <meta name="citation_author" content="Zhang, Aidong">
  <meta name="citation_author" content="Rehg, James M.">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="COLM 2025">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2406.10424">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <!-- TODO: Replace with your paper title and authors -->
  <title>What is the Visual Cognition Gap between Humans and Multimodal LLMs?</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">What is the Visual Cognition Gap between Humans and Multimodal LLMs?</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://www.irohxucao.com/" target="_blank">Xu Cao</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://shenyifans.github.io/" target="_blank">Yifan Shen</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://bolinlai.github.io/" target="_blank">Bolin Lai</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://wenqian-ye.github.io/" target="_blank">Wenqian Ye</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://ysma.me/" target="_blank">Yuansheng Ma</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://neuroscience.illinois.edu/directory/profile/jheintz" target="_blank">Joerg Heintz</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://whatashot.github.io/" target="_blank">Jintai Chen</a><sup>5</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=sM_jWNkAAAAJ&hl=en" target="_blank">Meihuan Huang</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=dPPOpHUAAAAJ&hl=en" target="_blank">Jianguo Cao</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.virginia.edu/~az9eg/website/home.html" target="_blank">Aidong Zhang</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://rehg.org/" target="_blank">James M. Rehg</a><sup>1</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">
                      <sup>1</sup> Department of Computer Science, University of Illinois at Urbana-Champaign<br>
                      <sup>2</sup> College of Computing, Georgia Institute of Technology<br>
                      <sup>3</sup> Department of Computer Science, University of Virginia<br>
                      <sup>4</sup> Digital Twin Lab, Purdue University<br>
                      <sup>5</sup> HKUST (Guangzhou)<br>
                      <sup>6</sup> Department of Rehabilitation Medicine, Shenzhen Childrenâ€™s Hospital<br>
                      <span style="font-size: 1.5rem; font-weight: bold;">COLM 2025</span>
                    </span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2406.10424" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.10424" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

               <!-- TODO: Replace with your GitHub repository URL -->
               <span class="link-block">
                <a href="https://github.com/PediaMedAI/Cognition-MLLM" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://huggingface.co/datasets/IrohXu/VCog-Bench" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  ðŸ¤—
                </span>
                <span>Benchmark</span>
              </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified" style="color: #000;">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Recently, Multimodal Large Language Models (MLLMs) and Vision Language Models (VLMs) have shown great promise in language-guided perceptual tasks such as recognition, segmentation, and object detection. However, their effectiveness in addressing visual cognition problems that require high-level multi-image reasoning and visual working memory is not well-established. One such challenge is matrix reasoning - the cognitive ability to discern relationships among patterns in a set of images and extrapolate to predict subsequent patterns. This skill is crucial during the early neurodevelopmental stages of children. Inspired by the matrix reasoning tasks in Raven's Progressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC), we propose a new dataset MaRs-VQA to evaluate the visual cognition capability of MLLMs and compare their performance with existing human visual cognition studies. Based on the training data of MaRs-VQA, we also finetune a baseline model Qwen2-VCog with multi-stage cognition reasoning annotations. Our comparative experiments with different baselines reveal a gap between MLLMs and human intelligence, highlighting the visual cognitive limitations of current MLLMs. We believe that the public release of MaRs-VQA and the Qwen2-VCog baseline model will drive progress toward the next generation of MLLMs with human-like visual cognition abilities. 
            MaRs-VQA is available at <a href="http://huggingface.co/datasets/IrohXu/VCog-Bench"><b>this http URL</b></a>. The training code of Qwen2-VCog is available at <a href="http://github.com/IrohXu/Cognition-MLLM"><b>this http URL</b></a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section" style="background-color:#dadada81">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">âœ… Contributions</h2>
        <div class="content has-text-justified" style="color: #000;">
          <ul class="fa-ul">
            <li><b>Cognitive Supersensing.</b> We propose Cognitive Supersensing, a latent-space reasoning-and-learning framework with supervised fine-tuning and reinforcement learning stages that endows MLLMs with visual imagery capability by aligning semantic reasoning with latent visual world modeling.</li>
            <br>
            <li><b>CogSense-Bench.</b> We conducted supervised fine-tuning (SFT) of Qwen2-VL using our annotated cognitive reasoning data. Our results indicate that fine-tuning enhances Qwen2-VL's accuracy on MaRs-VQA to match human performance levels; however, its generalization capabilities remain limited.</li>
            <br>
            <li><b>Visual Cognition Research.</b> Our thorough experiments qualitatively reveal the visual cognition gap between MLLMs and humans in matrix reasoning problems. We also show additional insights of deficiencies in MLLMs, which can inspire more future investigations in model design.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">ðŸ§¾ MaRs-VQA Dataset</h2>
        <figure style="text-align: center;">
          <img src="static/images/marsvqa1.png" style="width: 100%; height: auto;"/>
          <figcaption style="font-style: normal; color: gray;">
            Comparison of recently released zero-shot matrix reasoning datasets to evaluate MLLMs.
          </figcaption>
        </figure>
        <br>
        <div class="content has-text-justified" style="color:#000;">
          <p>
            The MaRs-VQA dataset is designed to evaluate the zero-shot abstract reasoning capabilities of MLLMs through various matrix reasoning VQA tasks. The images in MaRs-VQA are sourced from the questionnaires in Matrix Reasoning Item Bank, which is created by psychologists including 18 sets of abstract reasoning questionnaires (80 instances in each set) for non-verbal abstract reasoning assessment of adolescents and adults. Each item presents an incomplete 3 x 3 matrix of abstract shapes, requiring participants to identify relationships among the shapes. Then, we create VQA annotations in the images from all questionnaires.
          </p>
        </div>
        <figure style="text-align: center;">
          <img src="static/images/marsvqa2.png" style="width: 100%; height: auto;"/>
          <figcaption style="font-style: normal; color: gray;">
            Explanation of Difficulty Levels in MaRs-VQA.
          </figcaption>
        </figure>
        <br>
        
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">ðŸ’»ï¸Ž SFT for VLM</h2>
        <figure style="text-align: center;">
          <img src="static/images/vlm_framework.png" style="width: 50%; height: auto;"/>
          <figcaption style="font-style: normal; color: gray;">
            Supervised fine-tuning VLM to generate two-section format for matrix reasoning problem.
          </figcaption>
        </figure>
        <br>
        <div class="content has-text-justified" style="color:#000;">
          <p>
            To enhance the reasoning capabilities of vision-language models (VLMs), we leverage the reasoning responses generated using the two-section CoT format as training data. Specifically, the step-by-step reasoning annotations in MaRs-VQA are used as supervision signals during fine-tuning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">ðŸ“Š Experiments Results</h2>
        <div class="content has-text-justified" style="color:#000;">
          <p>
            We conducted evaluations on <b>MaRs-VQA</b> test set for in-domain performance and a subset of <b>RAVEN</b> for Out-of-Domain (OOD) performance.
          </p>
        </div>
        <figure style="text-align: center;">
          <img src="static/images/result.png" style="width: 100%; height: auto;"/>
          <figcaption style="font-style: normal; color: gray;">
            Benchmarking experiments on multi-image reasoning in MaRs-VQA (in-domain) and RAVEN (OOD). zero-shot means only provide the model system prompt about the matrix reasoning task definition. CoT denotes the implementation in section 5.1. The results are averaged over three runs with three different random seeds. * of Qwen2-VCog denotes CoT performance without finetuning.
          </figcaption>
        </figure>
        <br>
        <div class="content has-text-justified" style="color:#000;">
          <p>
            We also compare the different CoT strategies: raw step-by-step CoT with hint, CoT reasoning with few-shot sample and multi-round CoT reasoning.
          </p>
        </div>
        <figure style="text-align: center;">
          <img src="static/images/ablation1.png" style="width: 50%; height: auto;"/>
          <figcaption style="font-style: normal; color: gray;">
            Ablation on few-shot sample CoT and multi-round CoT for GPT-4o in MaRs-VQA. GPT-o1 still outperforms GPT-4o with different CoT strategy.
          </figcaption>
        </figure>
        <br>
        <div class="content has-text-justified" style="color:#000;">
          <p>
            To more precisely quantify the impact of explicit step-by-step reasoning supervision in Supervised Fine-Tuning (SFT), we conducted an ablation study in which the reasoning strings were omitted from the MaRs-VQA training data.
          </p>
        </div>
        <figure style="text-align: center;">
          <img src="static/images/ablation2.png" style="width: 70%; height: auto;"/>
          <figcaption style="font-style: normal; color: gray;">
            Effect of reasoning strings in SFT.
          </figcaption>
        </figure>
        <br>
        <div class="content has-text-justified" style="color:#000;">
          <p>
            We also analyze the relationship between matrix reasoning accuracy and model scale.
          </p>
        </div>
        <figure style="text-align: center;">
          <img src="static/images/scale.png" style="width: 100%; height: auto;"/>
          <figcaption style="font-style: normal; color: gray;">
            There is still a substantial gap between MLLM's (zero-shot CoT or SFT training) matrix reasoning capability and human's (zero-shot). Bubble size corresponds to the model size. As we donâ€™t know the exact size of closed-source MLLMs, we set all of them to the largest value by default. The model size of human refers to the number of neurons (86B) in human's brain.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">Citation</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@misc{cao2024visualcognitiongaphumans,
      title={What is the Visual Cognition Gap between Humans and Multimodal LLMs?}, 
      author={Xu Cao and Bolin Lai and Wenqian Ye and Yunsheng Ma and Joerg Heintz and Jintai Chen and Jianguo Cao and James M. Rehg},
      year={2024},
      eprint={2406.10424},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.10424}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
